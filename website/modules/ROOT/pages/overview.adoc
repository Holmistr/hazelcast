
[[hazelcast-overview]]
= Overview

This chapter describes Hazelcast's architecture along with its
partitioning mechanism, use cases and topology.

[[hazelcast-imdg-architecture]]
== Architecture

You can see the features for all Hazelcast IMDG editions in the following
architecture diagram.

image::HazelcastArchitecture.png[Hazelcast Architecture]

NOTE: You can see small "HD" boxes for some features in the above diagram.
Those features can use High-Density (HD) Memory Store when it is available.
It means if you have Hazelcast IMDG Enterprise HD, you can use those features with HD Memory Store.

For more information on Hazelcast IMDG's Architecture, see the white paper
link:https://hazelcast.com/resources/architects-view-hazelcast/[An Architect's View of Hazelcast^].

[[hazelcast-topology]]
== Topology

You can deploy a Hazelcast cluster in two ways: Embedded or Client/Server.

If you have an application whose main focal point is asynchronous or high performance
computing and lots of task
executions, then Embedded deployment is the preferred way. In Embedded deployment,
members include both the application and Hazelcast data and services. The advantage
of the Embedded deployment is having a low-latency data access.

See the below illustration.

image::Embedded.png[Embedded Deployment]

In the Client/Server deployment, Hazelcast data and services are centralized in one or
more server members and they are accessed by the application through clients.
You can have a cluster of server members that can be independently created and scaled.
Your clients communicate with
these members to reach to Hazelcast data and services on them.

See the below illustration.

image::ClientServer.png[Client/Server Deployment]

Hazelcast provides native clients (Java, .NET and C++), Memcache and REST clients, Scala,
Python and Node.js client implementations.

Client/Server deployment has advantages including more predictable and reliable Hazelcast
performance, easier identification of problem causes and, most importantly, better scalability.
When you need to scale in this deployment type, just add more Hazelcast server members. You
can address client and server scalability concerns separately.

Note that Hazelcast **member** libraries are available only in Java. Therefore, embedding a
member to a business service, it is only possible with Java. Applications written in other
languages (.NET, C++, Node.js, etc.) can use Hazelcast client libraries to access the cluster.
See the <<hazelcast-clients, Hazelcast Clients chapter>> for information on the clients and other language implementations. 

If you want low-latency data access, as in the Embedded deployment, and you also want the
scalability advantages of the Client/Server deployment, you can consider defining Near Caches
for your clients. This enables the frequently used data to be kept in the client's local memory.
See the <<configuring-client-near-cache, Configuring Client Near Cache section>>.

[[sharding-in-hazelcast]]
== Sharding

Hazelcast shards are called **partitions**. By default, Hazelcast has 271 partitions.
Given a key, we serialize, hash and mod it with the number of partitions to find
the partition which the key belongs to. The partitions themselves are
distributed equally among the members of the cluster. Hazelcast also creates the
backups of partitions and distributes
them among members for redundancy.

NOTE: See the <<data-partitioning, Data Partitioning section>> for more
information on how Hazelcast partitions
your data.

[[data-partitioning]]
== Data Partitioning

As you read in the <<sharding-in-hazelcast, Sharding in Hazelcast section>>, Hazelcast
shards are called Partitions. Partitions are memory segments that can contain hundreds
or thousands of data entries each, depending on the memory capacity of your system. Each
Hazelcast partition can have multiple replicas, which are distributed among the cluster
members. One of the replicas becomes the `primary` and other replicas are called `backups`.
Cluster member which owns `primary` replica of a partition is called `partition owner`.
When you read or write a particular data entry, you transparently talk to the owner of
the partition that contains the data entry.

By default, Hazelcast offers 271 partitions. When you start a cluster with a single member,
it owns all of 271 partitions (i.e., it keeps primary replicas for 271 partitions). The following
illustration shows the partitions in a Hazelcast cluster with single member.

image::NodePartition.jpg[Single Member with Partitions]

When you start a second member on that cluster (creating a Hazelcast cluster with two members),
the partition replicas are distributed as shown in the illustration here.

NOTE: Partition distributions in the below illustrations are shown for the sake of simplicity and
for descriptive purposes. Normally, the partitions are not distributed in any order, as they are
shown in these illustrations, but are distributed randomly (they do not have to be sequentially
distributed to each member). The important point here is that Hazelcast equally distributes the
partition primaries and their backup replicas among the members.

image::BackupPartitions.jpg[Cluster with Two Members - Backups are Created]

In the illustration, the partition replicas with black text are primaries and the partition replicas
with blue text are backups. The first member has primary replicas of 135 partitions (black) and
each of these partitions are backed up in the second member (i.e., the second member owns the
backup replicas) (blue). At the same time, the first member also has the backup replicas of
the second member's primary partition replicas.

As you add more members, Hazelcast moves some of the primary and backup partition replicas to
the new members one by one, making all members equal and redundant. Thanks to the consistent
hashing algorithm, only the minimum amount of partitions are moved to scale out Hazelcast. The
following is an illustration of the partition replica distributions in a Hazelcast cluster with four members.

image::4NodeCluster.jpg[Cluster with Four Members]

Hazelcast distributes partitions' primary and backup replicas equally among the members of the
cluster. Backup replicas of the partitions are maintained for redundancy.

NOTE: Your data can have multiple copies on partition primaries and backups, depending on your
backup count. See the <<backing-up-maps, Backing Up Maps section>>.

Hazelcast also offers lite members. These members do not own any partition. Lite members are
intended for use in computationally-heavy task executions and listener registrations. Although
they do not own any partitions,
they can access partitions that are owned by other members in the cluster.

NOTE: See the <<enabling-lite-members, Enabling Lite Members section>>.

[[how-the-data-is-partitioned]]
=== How the Data is Partitioned

Hazelcast distributes data entries into the partitions using a hashing algorithm. Given an object
key (for example, for a map) or an object name (for example, for a topic or list):

* the key or name is serialized (converted into a byte array)
* this byte array is hashed
* the result of the hash is mod by the number of partitions.

The result of this modulo - *MOD(hash result, partition count)* -  is the partition in which the
data will be stored, that is the **partition ID**. For ALL members you have in your cluster, the
partition ID for a given key is always the same.

[[partition-table]]
=== Partition Table

When you start a member, a partition table is created within it. This table stores the partition
IDs and the cluster members to which they belong. The purpose of this table is to make all members
(including lite members) in the cluster aware of this information, making sure that each member
knows where the data is.

The oldest member in the cluster (the one that started first) periodically sends the partition
table to all members. In this way each member in the cluster is informed about any changes to
partition ownership. The ownerships may be changed when, for example, a new member joins the
cluster, or when a member leaves the cluster.

NOTE: If the oldest member of the cluster goes down, the next oldest member sends the partition
table information to the other ones.

You can configure the frequency (how often) that the member sends the partition table the information
by using the `hazelcast.partition.table.send.interval` system property. The property is set to every
15 seconds by default.

[[repartitioning]]
=== Repartitioning

Repartitioning is the process of redistribution of partition ownerships. Hazelcast performs the
repartitioning when a member joins or leaves the cluster.

In these cases, the partition table in the oldest member is updated with the new partition
ownerships. Note that if a lite member joins or leaves a cluster, repartitioning is not triggered
since lite members do not own any partitions.

[[use-cases]]
== Use Cases

Hazelcast can be used:

* to share server configuration/information to see how a cluster performs
* to cluster highly changing data with event notifications, e.g., user based events, and
to queue and distribute background tasks
* as a simple Memcache with Near Cache
* as a cloud-wide scheduler of certain processes that need to be performed on some members
* to share information (user information, queues, maps, etc.) on the fly with multiple
members in different installations under OSGI environments
* to share thousands of keys in a cluster where there is a web service interface on an
application server and some validation
* as a distributed topic (publish/subscribe server) to build scalable chat servers for smartphones
* as a front layer for a Cassandra back-end
* to distribute user object states across the cluster, to pass messages between objects
and to share system data structures (static initialization state, mirrored objects, object
identity generators)
* as a multi-tenancy cache where each tenant has its own map
* to share datasets, e.g., table-like data structure, to be used by applications
* to distribute the load and collect status from Amazon EC2 servers where the front-end is
developed using, for example, Spring framework
* as a real-time streamer for performance detection
* as storage for session data in web applications (enables horizontal scalability of the web application).

[[resources]]
== Resources

* Hazelcast source code can be found at link:https://github.com/hazelcast/hazelcast[GitHub/Hazelcast^].
This is also where you can contribute and report issues.
* Hazelcast API can be found at link:https://docs.hazelcast.org/docs/latest-dev/javadoc/[Hazelcast.org/docs/Javadoc^].
* Code samples can be downloaded from link:https://hazelcast.org/imdg/download/[Hazelcast.org/download^].
* More use cases and resources can be found at link:http://www.hazelcast.com[Hazelcast.com^].
* Questions and discussions can be posted at the link:https://groups.google.com/forum/#!forum/hazelcast[Hazelcast mail group^].
